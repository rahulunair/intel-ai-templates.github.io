name: Phi-3-mini-4k
title: Phi-3 mini 4k
model_id: microsoft/Phi-3-mini-4k-instruct
description: "Microsoft's Phi-3-mini-4k model running on Text Generation Inference (TGI). A compact yet powerful model optimized for reasoning, coding, and structured tasks."

license_notice: "Before using this model, please review the license terms and usage rights on the Hugging Face model page."

# TGI specific configuration
tgi: true
architecture_diagram: |
  flowchart LR
      Client([Client])
      Traefik[Traefik Proxy]
      Auth[Auth Service]
      TGI[TGI Service]

      Client --> Traefik
      Traefik --> Auth
      Auth --> Traefik
      Traefik --> TGI
      TGI --> Traefik
      Traefik --> Client

      subgraph Internal["Internal Network"]
          Traefik
          Auth
          TGI
      end

key_features:
  - "üîí Token-based authentication with automatic ban after failed attempts"
  - "üö¶ Rate limiting (global: 10 req/s, per-IP: 10 req/s)"
  - "üõ°Ô∏è Security headers and IP protection"
  - "üîÑ Health monitoring and automatic recovery"
  - "üöÄ Optimized for Intel GPUs"

# Model specific information
context_window: "4096 tokens"
strengths:
  - "Code generation"
  - "Step-by-step reasoning"
  - "Math problems"
  - "Technical explanations"

use_cases:
  - "Writing and debugging code"
  - "Solving mathematical problems"
  - "Explaining technical concepts"
  - "Logical reasoning tasks"

configuration: |
  MODEL_NAME=phi-3-mini-4k-tgi
  MODEL_ID=microsoft/Phi-3-mini-4k-instruct
  TGI_VERSION=2.4.0-intel-xpu
  PORT=8081
  SHM_SIZE=2g
  MAX_CONCURRENT_REQUESTS=1
  MAX_BATCH_SIZE=1
  MAX_TOTAL_TOKENS=4096
  MAX_INPUT_LENGTH=2048
  MAX_WAITING_TOKENS=10

examples:
  - title: "Code Generation"
    code: |
      curl -X POST http://localhost:8081/generate \
        -H 'Content-Type: application/json' \
        -d '{
          "inputs": "Write a Python function that calculates the factorial of a number. Include comments explaining the logic:",
          "parameters": {
            "max_new_tokens": 150,
            "temperature": 0.2
          }
        }'

  - title: "Step-by-Step Math"
    code: |
      curl -X POST http://localhost:8081/generate \
        -H 'Content-Type: application/json' \
        -d '{
          "inputs": "Solve step by step: A store offers a 15% discount on a $80 item. What is the final price after tax if the tax rate is 8%?",
          "parameters": {
            "max_new_tokens": 200,
            "temperature": 0.1
          }
        }'

best_practices:
  - "Use temperature 0.1-0.2 for code and math"
  - "Include 'step by step' for complex problems"
  - "For coding tasks, specify programming language, desired functionality, and any specific requirements"
  - "Break down complex queries into smaller, logical steps"

deployment_steps:
  - title: "Create Account"
    description: "Create a standard account on Intel Tiber AI Cloud"
    link: "https://cloud.intel.com"
    link_text: "Visit Intel Cloud Portal"

  - title: "Select Hardware"
    description: "Select an Intel Data Center Max Series GPU VM"

  - title: "Clone Repository"
    description: "Clone the TGI repository"
    code: "git clone https://github.com/tiberaicommunity/xpu_tgi"

  - title: "Generate Token"
    description: "Generate authentication token for secure access"
    code: "python utils/generate_token.py"

  - title: "Start Model"
    description: "Start the Phi-3 model"
    code: "./start.sh phi-3-mini-4k"

  - title: "Test Model"
    description: "Make a test request to verify deployment"
    code: |
      curl -X POST http://localhost:8000/generate \
        -H "Authorization: Bearer YOUR_TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"inputs": "Hello, how are you?", "parameters": {"max_new_tokens": 50}}'

# Tags for filtering
tags:
  - label: "Transformer"
    color: "#3B82F6"
  - label: "4K context"
    color: "#10B981"
  - label: "Instruction"
    color: "#8B5CF6"
  - label: "TGI"
    color: "#EF4444" 